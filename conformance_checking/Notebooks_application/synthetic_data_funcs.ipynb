{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Event Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating intitial log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import timedelta\n",
    "from pm4py.objects.petri_net.importer import importer as pnml_importer\n",
    "\n",
    "petri_net, initial_marking, final_marking = pnml_importer.apply('/home/borna/ProcessMining/Assignments/Process_Mining/conformance_checking/data/BPI2017Denied_petriNet.pnml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "\n",
    "def generate_synthetic_log(petri_net, initial_marking, final_marking):\n",
    "    \"\"\"\n",
    "    Generate a synthetic event log from a Petri net and convert it to a Pandas DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    petri_net : PetriNet\n",
    "        The Petri net model.\n",
    "    initial_marking : Marking\n",
    "        The initial marking of the Petri net.\n",
    "    final_marking : Marking\n",
    "        The final marking of the Petri net.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the synthetic event log.\n",
    "    \"\"\"\n",
    "    # Generate synthetic event log from the Petri net\n",
    "    synth_log = pm4py.sim.play_out(petri_net, initial_marking, final_marking)\n",
    "\n",
    "    # Convert synthetic log to Pandas DataFrame\n",
    "    log_data = []\n",
    "    for trace in synth_log:\n",
    "        trace_id = trace.attributes['concept:name']  # Access attributes as object properties\n",
    "        for event in trace:\n",
    "            log_data.append({\n",
    "                'trace_id': trace_id,\n",
    "                'activity': event['concept:name'],  # Event attributes are accessed this way\n",
    "                'timestamp': event['time:timestamp']\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame for the synthetic event log\n",
    "    synth_log_df = pd.DataFrame(log_data)\n",
    "    return synth_log_df\n",
    "\n",
    "# Example usage\n",
    "synth_log_df = generate_synthetic_log(petri_net, initial_marking, final_marking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating adjusted timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "def adjust_timestamps(df, trace_column='trace_id', timestamp_column='timestamp', min_increment=2400, max_increment=6000):\n",
    "    \"\"\"\n",
    "    Adjust timestamps within each trace to add realistic time differences while maintaining chronological order.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The event log DataFrame containing traces with activities and timestamps.\n",
    "    trace_column : str\n",
    "        The column name representing trace identifiers.\n",
    "    timestamp_column : str\n",
    "        The column name representing activity timestamps.\n",
    "    min_increment : int\n",
    "        Minimum increment (in seconds) between consecutive activities.\n",
    "    max_increment : int\n",
    "        Maximum increment (in seconds) between consecutive activities.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The updated DataFrame with adjusted timestamps.\n",
    "    \"\"\"\n",
    "    # Copy the DataFrame to avoid modifying the original data\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert timestamps to pandas datetime if not already\n",
    "    df[timestamp_column] = pd.to_datetime(df[timestamp_column], utc=True)\n",
    "    \n",
    "    # Group by trace and process each trace\n",
    "    for trace_id, group in df.groupby(trace_column):\n",
    "        # Sort the group by timestamp to ensure order\n",
    "        group = group.sort_values(by=timestamp_column)\n",
    "        adjusted_timestamps = []\n",
    "        current_time = group.iloc[0][timestamp_column]  # Start with the first timestamp\n",
    "        \n",
    "        # Generate adjusted timestamps for the trace\n",
    "        for _ in range(len(group)):\n",
    "            adjusted_timestamps.append(current_time)\n",
    "            # Increment the current time by a random value within the range\n",
    "            increment = timedelta(seconds=random.randint(min_increment, max_increment))\n",
    "            current_time += increment\n",
    "        \n",
    "        # Update the original DataFrame with adjusted timestamps\n",
    "        df.loc[group.index, timestamp_column] = adjusted_timestamps\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example of how to use this function (replace 'synth_log_df' with your actual DataFrame variable name)\n",
    "# Updated timestamps\n",
    "synth_log_df_adjusted = adjust_timestamps(synth_log_df, trace_column='trace_id', timestamp_column='timestamp')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def introduce_synthetic_deviations_W(\n",
    "    log_df: pd.DataFrame,\n",
    "    activities_to_skip: list,\n",
    "    deviation_ratio: float = 0.3,\n",
    "    time_reduction_range: tuple = (3600, 10800),  # Time reduction range in seconds\n",
    "    seed: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Introduces synthetic deviations by skipping user-specified activities in a subset of traces.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    log_df : pd.DataFrame\n",
    "        The synthetic event log as a DataFrame with columns ['trace_id', 'activity', 'timestamp'].\n",
    "    activities_to_skip : list\n",
    "        List of activities to skip (remove) from traces.\n",
    "    deviation_ratio : float, optional\n",
    "        The ratio of traces containing the specified activity to modify (default is 0.3, i.e., 30%).\n",
    "    time_reduction_range : tuple, optional\n",
    "        Range of time (in seconds) to reduce for subsequent events (default is (60, 3600)).\n",
    "    seed : int, optional\n",
    "        Random seed for reproducibility (default is 42).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A modified event log with synthetic deviations introduced.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    modified_log = log_df.copy()\n",
    "\n",
    "    for activity in activities_to_skip:\n",
    "        # Identify traces containing the activity\n",
    "        traces_with_activity = modified_log[modified_log['activity'] == activity]['trace_id'].unique()\n",
    "\n",
    "        # Randomly select traces to modify based on the deviation ratio\n",
    "        num_traces_to_modify = int(len(traces_with_activity) * deviation_ratio)\n",
    "        traces_to_modify = np.random.choice(traces_with_activity, num_traces_to_modify, replace=False)\n",
    "\n",
    "        for trace_id in traces_to_modify:\n",
    "            # Filter the trace\n",
    "            trace = modified_log[modified_log['trace_id'] == trace_id].copy()\n",
    "\n",
    "            # Find indices of activities to be removed\n",
    "            indices_to_remove = trace[trace['activity'] == activity].index\n",
    "\n",
    "            # Determine the first affected index\n",
    "            first_affected_index = indices_to_remove.min()\n",
    "\n",
    "            # Remove all occurrences of the activity\n",
    "            trace = trace.drop(indices_to_remove)\n",
    "\n",
    "            # Generate a random time reduction in seconds\n",
    "            time_reduction = pd.Timedelta(seconds=np.random.uniform(*time_reduction_range))\n",
    "\n",
    "            # Adjust timestamps for activities after the last removed activity\n",
    "            if not trace.empty and first_affected_index is not None:\n",
    "                # Identify indices of events following the removed activities\n",
    "                subsequent_indices = trace.index[trace.index > first_affected_index]\n",
    "\n",
    "                # Adjust their timestamps\n",
    "                trace.loc[subsequent_indices, 'timestamp'] -= time_reduction\n",
    "\n",
    "            # Replace the trace in the modified log\n",
    "            modified_log = modified_log[modified_log['trace_id'] != trace_id]\n",
    "            modified_log = pd.concat([modified_log, trace])\n",
    "\n",
    "    # Sort the modified log by trace_id and timestamp\n",
    "    modified_log = modified_log.sort_values(by=['trace_id', 'timestamp']).reset_index(drop=True)\n",
    "    return modified_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating deviated log and allignments example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "synthetic_log_deviat_test = introduce_synthetic_deviations_W(\n",
    "    log_df=synth_log_df_adjusted,\n",
    "    activities_to_skip=['W-Call-incomplete-files-suspend', 'A-Incomplete', 'W-Assess-potential-fraud-suspend'],\n",
    "    deviation_ratio=0.8,\n",
    "    time_reduction_range=(3600, 9600),\n",
    "    seed=12\n",
    ")\n",
    "\n",
    "from pm4py.objects.petri_net.importer import importer as pnml_importer\n",
    "\n",
    "petri_net, initial_marking, final_marking = pnml_importer.apply('/home/borna/ProcessMining/Assignments/Process_Mining/conformance_checking/data/BPI2017Denied_petriNet.pnml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Allignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_alignments_with_trace_ids(log_df, alignments):\n",
    "    \"\"\"\n",
    "    Map alignments to their respective trace IDs from the event log.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    log_df : pd.DataFrame\n",
    "        Event log as a DataFrame containing 'case:concept:name'.\n",
    "    alignments : List\n",
    "        List of alignment dictionaries from the conformance checking.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Dict[str, Any]]\n",
    "        A list where each element contains:\n",
    "            - 'trace_id': The trace ID\n",
    "            - 'alignment': The corresponding alignment\n",
    "    \"\"\"\n",
    "    # Ensure trace IDs are unique and in order\n",
    "    unique_trace_ids = log_df['case:concept:name'].unique()\n",
    "\n",
    "    if len(unique_trace_ids) != len(alignments):\n",
    "        raise ValueError(\"Mismatch between the number of traces in the log and alignments!\")\n",
    "\n",
    "    # Map alignments to trace IDs\n",
    "    alignments_with_trace_ids = [\n",
    "        {\"trace_id\": trace_id, \"alignment\": alignment}\n",
    "        for trace_id, alignment in zip(unique_trace_ids, alignments)\n",
    "    ]\n",
    "\n",
    "    return alignments_with_trace_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate trace encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trace_encoding(alignments_with_trace_ids):\n",
    "    \"\"\"\n",
    "    Generate trace encodings and associate them with trace IDs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alignments_with_trace_ids : List[Dict[str, Any]]\n",
    "        A list where each element contains a trace ID and its corresponding alignment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Dict[str, Any]]\n",
    "        A list where each element includes a trace ID and its move counts.\n",
    "    \"\"\"\n",
    "    trace_encodings = []\n",
    "\n",
    "    for item in alignments_with_trace_ids:\n",
    "        trace_id = item['trace_id']\n",
    "        alignment = item['alignment']\n",
    "\n",
    "        # Initialize move count for the trace\n",
    "        move_count = {}\n",
    "        for log_activity, model_activity in alignment['alignment']:\n",
    "            if log_activity != \">>\" and log_activity not in move_count:\n",
    "                move_count[log_activity] = {'log_moves': 0, 'model_moves': 0}\n",
    "            elif model_activity != \">>\" and model_activity not in move_count:\n",
    "                move_count[model_activity] = {'log_moves': 0, 'model_moves': 0}\n",
    "\n",
    "            if log_activity == \">>\":\n",
    "                move_count[model_activity]['model_moves'] += 1\n",
    "            elif model_activity == \">>\":\n",
    "                move_count[log_activity]['log_moves'] += 1\n",
    "\n",
    "        trace_encodings.append({'trace_id': trace_id, 'move_count': move_count})\n",
    "\n",
    "    return trace_encodings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE THIS FUNCTION RENAMES THE trace_number column in csv file to trace_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe_for_decision_tree_with_throughput(log_df, trace_encodings, save_path):\n",
    "    \"\"\"\n",
    "    Create a DataFrame for decision tree analysis from trace encodings and throughput times.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    log_df : pd.DataFrame\n",
    "        The synthetic event log as a DataFrame.\n",
    "    trace_encodings : List[Dict[str, Any]]\n",
    "        A list of trace encodings with their IDs.\n",
    "    save_path : str\n",
    "        The path to save the resulting CSV file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Calculate throughput time for each trace\n",
    "    throughput_times = log_df.groupby('case:concept:name').agg(\n",
    "        first_event=('time:timestamp', 'min'),\n",
    "        last_event=('time:timestamp', 'max')\n",
    "    )\n",
    "    throughput_times['case:throughput_time'] = throughput_times['last_event'] - throughput_times['first_event']\n",
    "    throughput_times = throughput_times[['case:throughput_time']].reset_index()\n",
    "\n",
    "    # Flatten trace encodings and include trace IDs\n",
    "    flattened_traces = []\n",
    "    for item in trace_encodings:\n",
    "        trace_id = item['trace_id']\n",
    "        move_count = item['move_count']\n",
    "\n",
    "        flat_trace = {'trace_id': trace_id}\n",
    "        for activity, moves in move_count.items():\n",
    "            if activity is not None:\n",
    "                activity = activity.replace(' ', '_')\n",
    "            flat_trace[f'{activity}_log_moves'] = moves['log_moves']\n",
    "            flat_trace[f'{activity}_model_moves'] = moves['model_moves']\n",
    "        flattened_traces.append(flat_trace)\n",
    "\n",
    "    df_flattened = pd.DataFrame(flattened_traces)\n",
    "\n",
    "    # Merge with throughput time\n",
    "    df = pd.merge(df_flattened, throughput_times, left_on='trace_id', right_on='case:concept:name')\n",
    "    df = df.drop('case:concept:name', axis=1)\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(save_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
